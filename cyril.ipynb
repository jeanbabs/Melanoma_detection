{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code pour manipuler fichiers et faire le data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idée : \n",
    "\n",
    "1) télécharger les données du petit dataset https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar?fbclid=IwAR2BEnfrRxPf-7Ri-eqdHkl4OjShyhY_42-R5Kx3vK4Z4SL0BmfOiCbMzF0\n",
    "\n",
    "2) Il faut trier ces images en 3 dossiers : normaux, anormaux et mélanomes\n",
    "    - Pour faire ça j'ai annoté le nom de chaque dossier suivant qu'il contenait une image d'un truc bénin, pas bénin, mélanome\n",
    "    ex : dossier Im058 devient m_Im058 car il contenait un mélanome\n",
    "    - comme ça tu peux facilement trier par préfixe et copier coller l'ensemble des dossier dans un nouveau dossier séparé\n",
    "    \n",
    "3) Maintenant que tu as les 3 dossiers de fait contenant chacun des dossiers, il faut extraire dans chaque sous dossier l'image et la stocker\n",
    "\n",
    "4) Une fois ça de fait, c'est plus facile car il faut compléter le dataset avec les images qui viennent de là : https://www.isic-archive.com/?fbclid=IwAR1z5lIE-A_zGL1ewbxjQG6Q7AfRq4mgIsRO_afgb6jVFpVK2f0ittcHHWI#!/onlyHeaderTop/gallery\n",
    "\n",
    "Tu peux par exemple télécharger 100 images de plus pour les bénins\n",
    "\n",
    "pour la catégorie 2 \"anormaux\" tu peux rajouter des images en filtrant avec \"unknown\" (mais prends des trucs pertinents), \"indeterminate\"\n",
    "\n",
    "pour la catégorie 3, en filtrant via malignant / melanomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour annoter facilement les dossiers j'ai créer 3 fichiers textes où sont stockés ligne par ligne le nom du dossier pour chaque cas genre melanomes.txt : IM058, IM069 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choppe les noms des fichiers pour les 3 catégories\n",
    "# possible qu'il te fasse chier avec le \\abnormal\n",
    "# si c'est le cas rename le car \\a fait bugguer\n",
    "\n",
    "\n",
    "melanomias = []\n",
    "\n",
    "with open(\"data\\PH2Dataset\\melanome.txt\", \"r\") as ifile:\n",
    "    for line in ifile:\n",
    "        melanomias.append(line.split(\"\\n\")[0])\n",
    "            \n",
    "            \n",
    "benins = []\n",
    "\n",
    "with open(\"data\\PH2Dataset\\benins.txt\", \"r\") as ifile:\n",
    "    for line in ifile:\n",
    "        benins.append(line.split(\"\\n\")[0])\n",
    "        \n",
    "suspicious = []\n",
    "\n",
    "with open(\"data\\PH2Dataset\\abnormal.txt\", \"r\") as ifile:\n",
    "    for line in ifile:\n",
    "        suspicious.append(line.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu te places dans le dossier où il y a tous les dossiers des images\n",
    "# tu itéres sur tous les dossiers et selon les cas tu modifies le préfixe\n",
    "\n",
    "dirpath = \"data\\PH2Dataset\\images\"\n",
    "for folder_name in os.listdir(\"data\\PH2Dataset\\images\\.\"):\n",
    "    if folder_name in melanomias:\n",
    "        newname =  \"m_\" + folder_name \n",
    "        os.rename(os.path.join(dirpath, folder_name), os.path.join(dirpath, newname))\n",
    "    elif folder_name in benins:\n",
    "        newname =  \"b_\" + folder_name \n",
    "        os.rename(os.path.join(dirpath, folder_name), os.path.join(dirpath, newname))\n",
    "    elif folder_name in suspicious:\n",
    "        newname =  \"s_\" + folder_name \n",
    "        os.rename(os.path.join(dirpath, folder_name), os.path.join(dirpath, newname))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là normalement si tout a bien marché tu as une longue liste de dossiers avec chacun un préfixe différent selon la caté, faut que tu crées **3 dossiers** appelés par ex : melanomas , suspicious, benins et tu copies colle dans chacun de ces dossiers les dossiers avec le préfixe qui va bien (tout ceux commencant par m_ dans le dossier melanoma etc)\n",
    "\n",
    "Et là faut extraire les images maintenant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'data\\\\melanoma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-cf9ed76b130d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdirpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data\\melanoma\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data\\melanoma_img\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0minput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_Dermoscopic_Image\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".bmp\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'data\\\\melanoma'"
     ]
    }
   ],
   "source": [
    "# tu te places dans le dossier melanoma et tu vas parcourir tous les dossiers et extraire les images dans un dossier appelé\n",
    "# melanoma_img\n",
    "# faut le faire pour chaque caté\n",
    "\n",
    "dirpath = \"data\\melanoma\"\n",
    "target_path = \"data\\melanoma_img\"\n",
    "for folder_name in os.listdir(dirpath):\n",
    "    input_path = os.path.join(dirpath, folder_name, folder_name.split(\"_\")[1] + \"_Dermoscopic_Image\", folder_name.split(\"_\")[1] + \".bmp\" )\n",
    "    shutil.copy2(input_path, target_path)\n",
    "\n",
    "\n",
    "dirpath = \"data\\suspicious\"\n",
    "target_path = \"data\\suspicious_img\"\n",
    "for folder_name in os.listdir(dirpath):\n",
    "    input_path = os.path.join(dirpath, folder_name, folder_name.split(\"_\")[1] + \"_Dermoscopic_Image\", folder_name.split(\"_\")[1] + \".bmp\" )\n",
    "    shutil.copy2(input_path, target_path)\n",
    "\n",
    "    \n",
    "dirpath = \"data\\benins\"\n",
    "target_path = \"data\\benins_img\"\n",
    "for folder_name in os.listdir(dirpath):\n",
    "    input_path = os.path.join(dirpath, folder_name, folder_name.split(\"_\")[1] + \"_Dermoscopic_Image\", folder_name.split(\"_\")[1] + \".bmp\" )\n",
    "    shutil.copy2(input_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu peux supprimer les listes de dossier et ne conserver que les 3 dossiers contenant les images des 3 catégories\n",
    "\n",
    "Si tout s'est bien passé tu as **3 dossiers (bénins, suspicious et mélanomes)** avec des images\n",
    "\n",
    "Maintenant il faut compléter ces images en telechargant d'autre images de la base ISIC cf ce que j'ai mis au début du notebook\n",
    "\n",
    "Ensuite il faudra convertir et stocker ces images sous forme d'array d'arrays avec numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prends en entrée le chemin du dossier contenant les images\n",
    "# par exemple le dossier contenant toutes les images de mélanomes\n",
    "# construit une liste avec le nom de chaque image par ex \"IM058.bmp\"\n",
    "# ensuite on stock dans un np.array les images 1) rescalées en 400 * 400 2) en gray scale\n",
    "# retourne un array d'array \n",
    "\n",
    "def from_images_to_npy_file(dirpath):\n",
    "    img_name_liste = []\n",
    "    for img_name in os.listdir(dirpath):\n",
    "        img_name_liste.append(img_name)\n",
    "        \n",
    "    x = np.array([resize(imread(dirpath + fname).mean(axis=2), (400, 400), mode='reflect', anti_aliasing=True) for fname in img_name_liste])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) convertir les images en array puis les stocker en array\n",
    "# 2) .dump permet de stocker ça en fichier .npy comme dans mars cratere\n",
    "\n",
    "array_melanomas = from_images_to_npy_file(\"data/moles_dataset/melanoma_img/\")\n",
    "array_melanomas.dump('data/moles_dataset/melanomias.npy')\n",
    "\n",
    "array_abnormal = from_images_to_npy_file(\"data/moles_dataset/abnormal_img/\")\n",
    "array_abnormal.dump('data/moles_dataset/abnormal.npy')\n",
    "\n",
    "array_normal = from_images_to_npy_file(\"data/moles_dataset/normal_img/\")\n",
    "array_normal.dump('data/moles_dataset/normal.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Bon là normalement si tout s'est bien passé tu as 3 objets .npy qui contiennent les images \n",
    "\n",
    "Maintenant il faut créer 3 fchiers CSV qui contiennent les y\n",
    "\n",
    "ça va dépendre du nombre d'images de chaque array mais par ex si array_melanomas contient 300 images donc shape = (300, ) où array_melanomas[0].shape = (300, 300) et bien il faut faire un fichier csv avec 300 fois \"2\" car ça sera de la classe 2 parmi (0, 1 ,2)\n",
    "\n",
    "Ensuite il faut uploader ces 6 fichiers sur un cloud, je compte sur ta connexion pour ça ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "création du dataset :\n",
    "\n",
    "récupérer les 3 fichiers contenant mélanomes - benins - suspicious\n",
    "\n",
    "coupe chaque fichier 70% - 15% - 15% (M1, M2, M3 pour le fichier mélanome par ex)\n",
    "\n",
    "stack \n",
    "- M1 - S1 - B1 <- train\n",
    "- M2 - S2 - B2 <- test\n",
    "- M3 - S3 - B3 <- validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "melanomas  = np.load(\"./data/melanoma.npy\")\n",
    "benins     = np.load(\"./data/benins.npy\")\n",
    "suspicious = np.load(\"./data/suspicious.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 400, 400)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melanomas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melanomas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 400, 400) (25, 400, 400) (25, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "M1, M2, M3 = np.split(melanomas, [158, 183])\n",
    "\n",
    "print(M1.shape, M2.shape, M3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324, 400, 400) (70, 400, 400) (70, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "B1, B2, B3 = np.split(benins, [324, 394])\n",
    "\n",
    "print(B1.shape, B2.shape, B3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 400, 400)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspicious.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 400, 400) (25, 400, 400) (25, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "S1, S2, S3 = np.split(suspicious, [137, 162])\n",
    "\n",
    "print(S1.shape, S2.shape, S3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id 0 -> 323 benin\n",
    "# 324 -> 460 suspicious\n",
    "# 461 -> end malin\n",
    "data_train = np.vstack((B1, S1, M1))\n",
    "\n",
    "# id 0 -> 69 benin\n",
    "# 70 -> 94 suspicious\n",
    "# 95 -> 119 malin\n",
    "data_test  = np.vstack((B2, S2, M2))\n",
    "data_validation  = np.vstack((B3, S3, M3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619, 400, 400)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data_train = [i for i in range(0, 619)]\n",
    "labels_data_train = [1 for i in range(0, 324)] + [2 for i in range(324, 461)] + [3 for i in range(461, 619)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': id_data_train, 'label': labels_data_train}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "df.to_csv(\"./data/data_train_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data_test = [i for i in range(0, 120)]\n",
    "labels_data_test = [1 for i in range(0, 70)] + [2 for i in range(70, 95)] + [3 for i in range(95, 120)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': id_data_test, 'label': labels_data_test}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "df.to_csv(\"./data/data_test_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dump('data/data_train.npy')\n",
    "data_test.dump('data/data_test.npy')\n",
    "data_validation.dump('data/data_validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\data.zip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "def main(dest_dir = 'data'):\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "    output_file = os.path.join(dest_dir, \"data.zip\")\n",
    "    \n",
    "    print(\"Starting download...\")\n",
    "    gdd.download_file_from_google_drive(file_id='1z1uVQpQvtjJocgbANCxfFaEVGEzYhnOc', dest_path=output_file, unzip=True)\n",
    "    print(\"=> File saved as {}\".format(output_file))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  np.load(\"./data/data_train.npy\")\n",
    "X_test =  np.load(\"./data/data_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/data_train_labels.csv\")\n",
    "ff = pd.read_csv(\"./data/data_test_labels.csv\")\n",
    "Y_train = np.array(df[\"label\"])\n",
    "Y_test = np.array(ff[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619, 400, 400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 400, 400)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 160000)\n",
    "X_test  = X_test.reshape(-1, 160000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619, 160000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.68      0.72        78\n",
      "           2       0.50      0.60      0.54        42\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       120\n",
      "   macro avg       0.63      0.64      0.63       120\n",
      "weighted avg       0.67      0.65      0.66       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(Y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "619/619 [==============================] - 40s 65ms/step - loss: 6.3342 - acc: 0.5460\n",
      "Epoch 2/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 6.7944 - acc: 0.5509\n",
      "Epoch 3/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 6.6767 - acc: 0.5574\n",
      "Epoch 4/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 6.6093 - acc: 0.5493\n",
      "Epoch 5/30\n",
      "619/619 [==============================] - 31s 50ms/step - loss: 6.4899 - acc: 0.5477\n",
      "Epoch 6/30\n",
      "619/619 [==============================] - 31s 50ms/step - loss: 6.3676 - acc: 0.5509\n",
      "Epoch 7/30\n",
      "619/619 [==============================] - 31s 50ms/step - loss: 6.1006 - acc: 0.5590\n",
      "Epoch 8/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 5.5431 - acc: 0.5606\n",
      "Epoch 9/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 2.5025 - acc: 0.4669\n",
      "Epoch 10/30\n",
      "619/619 [==============================] - 32s 51ms/step - loss: 1.2660 - acc: 0.5638\n",
      "Epoch 11/30\n",
      "619/619 [==============================] - 32s 52ms/step - loss: 1.0048 - acc: 0.5396\n",
      "Epoch 12/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.7706 - acc: 0.5767\n",
      "Epoch 13/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.6509 - acc: 0.6704\n",
      "Epoch 14/30\n",
      "619/619 [==============================] - 32s 51ms/step - loss: 0.6495 - acc: 0.6624\n",
      "Epoch 15/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.6226 - acc: 0.6591\n",
      "Epoch 16/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.6231 - acc: 0.6543\n",
      "Epoch 17/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.6189 - acc: 0.6494\n",
      "Epoch 18/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.6029 - acc: 0.6785\n",
      "Epoch 19/30\n",
      "619/619 [==============================] - 32s 51ms/step - loss: 0.5790 - acc: 0.6866\n",
      "Epoch 20/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.5806 - acc: 0.6785\n",
      "Epoch 21/30\n",
      "619/619 [==============================] - 32s 51ms/step - loss: 0.5998 - acc: 0.6834\n",
      "Epoch 22/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.5830 - acc: 0.6801\n",
      "Epoch 23/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.5792 - acc: 0.6882\n",
      "Epoch 24/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.5700 - acc: 0.6898\n",
      "Epoch 25/30\n",
      "619/619 [==============================] - 31s 51ms/step - loss: 0.5625 - acc: 0.7027\n",
      "Epoch 26/30\n",
      "619/619 [==============================] - 33s 53ms/step - loss: 0.5768 - acc: 0.6979\n",
      "Epoch 27/30\n",
      "619/619 [==============================] - 34s 55ms/step - loss: 0.5517 - acc: 0.7254\n",
      "Epoch 28/30\n",
      "619/619 [==============================] - 33s 54ms/step - loss: 0.5329 - acc: 0.7270\n",
      "Epoch 29/30\n",
      "619/619 [==============================] - 36s 59ms/step - loss: 0.5237 - acc: 0.7237\n",
      "Epoch 30/30\n",
      "619/619 [==============================] - 34s 55ms/step - loss: 0.5288 - acc: 0.7221\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "N = X_train.shape[1]\n",
    "H = 1200\n",
    "K = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(K))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(),\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=30, batch_size=64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.65        71\n",
      "           1       0.50      0.51      0.51        49\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       120\n",
      "   macro avg       0.58      0.58      0.58       120\n",
      "weighted avg       0.59      0.59      0.59       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, Y_test-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
